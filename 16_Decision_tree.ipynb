{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6eXQ8wUQt0B"
      },
      "outputs": [],
      "source": [
        "# Creating decision tree model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "# counter is used to count the frequency of each class\n",
        "from math import log2\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self):\n",
        "        self.tree = None\n",
        "\n",
        "    def entropy(self, y):\n",
        "        \"\"\"Calculate the entropy of a dataset.\"\"\"\n",
        "        class_counts = Counter(y)\n",
        "        total_instances = len(y)\n",
        "        entropy = 0\n",
        "        for count in class_counts.values():\n",
        "            prob = count / total_instances\n",
        "            entropy -= prob * log2(prob)\n",
        "        return entropy\n",
        "\n",
        "    def information_gain(self, X, y, feature_index):\n",
        "        \"\"\"Calculate the information gain of a split on a feature.\"\"\"\n",
        "        total_entropy = self.entropy(y)\n",
        "        values = [x[feature_index] for x in X]\n",
        "        unique_values = set(values)\n",
        "        weighted_entropy = 0\n",
        "        for value in unique_values:\n",
        "            subset_y = [y[i] for i in range(len(y)) if X[i][feature_index] == value]\n",
        "            prob = len(subset_y) / len(y)\n",
        "            weighted_entropy += prob * self.entropy(subset_y)\n",
        "        return total_entropy - weighted_entropy\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        \"\"\"Find the best feature to split on based on information gain.\"\"\"\n",
        "        num_features = len(X[0])\n",
        "        best_feature = None\n",
        "        max_info_gain = -1\n",
        "        for i in range(num_features):\n",
        "            info_gain = self.information_gain(X, y, i)\n",
        "            if info_gain > max_info_gain:\n",
        "                max_info_gain = info_gain\n",
        "                best_feature = i\n",
        "        return best_feature\n",
        "\n",
        "    def build_tree(self, X, y):\n",
        "        \"\"\"Recursively build the decision tree.\"\"\"\n",
        "        unique_classes = set(y)\n",
        "        if len(unique_classes) == 1:\n",
        "            return unique_classes.pop()\n",
        "\n",
        "        if len(X[0]) == 0:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_feature = self.best_split(X, y)\n",
        "        tree = {best_feature: {}}\n",
        "\n",
        "        values = set([x[best_feature] for x in X])\n",
        "        for value in values:\n",
        "            subset_X = [x[:best_feature] + x[best_feature+1:] for x in X if x[best_feature] == value]\n",
        "            subset_y = [y[i] for i in range(len(y)) if X[i][best_feature] == value]\n",
        "            subtree = self.build_tree(subset_X, subset_y)\n",
        "            tree[best_feature][value] = subtree\n",
        "\n",
        "        return tree\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self.build_tree(X, y)\n",
        "\n",
        "    def predict_sample(self, tree, sample):\n",
        "        if isinstance(tree, dict):\n",
        "            feature = list(tree.keys())[0]\n",
        "            value = sample[feature]\n",
        "            subtree = tree[feature].get(value)\n",
        "            if subtree is None:\n",
        "                return None\n",
        "            return self.predict_sample(subtree, sample)\n",
        "        else:\n",
        "            return tree\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self.predict_sample(self.tree, sample) for sample in X]\n",
        "\n",
        "\n",
        "# apply the model\n",
        "model = DecisionTree()\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knn6t556RiMv",
        "outputId": "7d69e18f-81db-4be3-8afb-28c6e5a338c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No', 'No', 'Yes', 'Yes', 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample dataset\n",
        "    X = [\n",
        "        ['Sunny', 'Hot', 'High', 'Weak'],\n",
        "        ['Sunny', 'Hot', 'High', 'Strong'],\n",
        "        ['Overcast', 'Hot', 'High', 'Weak'],\n",
        "        ['Rain', 'Mild', 'High', 'Weak'],\n",
        "        ['Rain', 'Cool', 'Normal', 'Weak']\n",
        "    ]\n",
        "    y = ['No', 'No', 'Yes', 'Yes', 'Yes']\n",
        "\n",
        "    # Convert categorical data to integers for simplicity\n",
        "    feature_mapping = {'Outlook': {'Sunny': 0, 'Overcast': 1, 'Rain': 2},\n",
        "                       'Temperature': {'Hot': 0, 'Mild': 1, 'Cool': 2},\n",
        "                       'Humidity': {'High': 0, 'Normal': 1},\n",
        "                       'Wind': {'Weak': 0, 'Strong': 1}}\n",
        "\n",
        "    X_encoded = [[feature_mapping[feature][value] for feature, value in zip(['Outlook', 'Temperature', 'Humidity', 'Wind'], row)] for row in X]\n",
        "\n",
        "    # Create and train the decision tree\n",
        "    tree = DecisionTree()\n",
        "    tree.fit(X_encoded, y)\n",
        "\n",
        "    # Predict on new data\n",
        "    test_data = [\n",
        "        ['Sunny', 'Mild', 'High', 'Strong'],\n",
        "        ['Rain', 'Cool', 'Normal', 'Weak']\n",
        "    ]\n",
        "    test_data_encoded = [[feature_mapping[feature][value] for feature, value in zip(['Outlook', 'Temperature', 'Humidity', 'Wind'], row)] for row in test_data]\n",
        "\n",
        "    predictions = tree.predict(test_data_encoded)\n",
        "    for sample, prediction in zip(test_data, predictions):\n",
        "        print(f\"Sample: {sample} => Prediction: {prediction}\")\n"
      ],
      "metadata": {
        "id": "FIceAYErRlcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "X = [\n",
        "        ['Sunny', 'Hot', 'High', 'Weak'],\n",
        "        ['Sunny', 'Hot', 'High', 'Strong'],\n",
        "        ['Overcast', 'Hot', 'High', 'Weak'],\n",
        "        ['Rain', 'Mild', 'High', 'Weak'],\n",
        "        ['Rain', 'Cool', 'Normal', 'Weak']\n",
        "    ]\n",
        "\n",
        "y = ['No', 'No', 'Yes', 'Yes', 'Yes']\n",
        "\n",
        "# Convert categorical data to integers for simplicity\n",
        "feature_mapping = {'Outlook': {'Sunny': 0, 'Overcast': 1, 'Rain': 2},\n",
        "                       'Temperature': {'Hot': 0, 'Mild': 1, 'Cool': 2},\n",
        "                       'Humidity': {'High': 0, 'Normal': 1},\n",
        "                       'Wind': {'Weak': 0, 'Strong': 1}}\n",
        "\n",
        "\n",
        "# make dataframe\n",
        "df = pd.DataFrame(X, columns=['Outlook', 'Temperature', 'Humidity', 'Wind'])\n",
        "df\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df = df.apply(le.fit_transform)\n",
        "\n",
        "\n",
        "# apply the sklearn model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(df, y)\n",
        "predictions = model.predict(df)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ao9852SAx8",
        "outputId": "bccbe40a-fdeb-49bf-c6e6-2a1ced14aa40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'No' 'Yes' 'Yes' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fp6TyX6dtDzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLIYNImetDwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How decision tree works\n",
        "# 1. Initialize the decision tree\n",
        "# 2. Calculate the information gain for each feature\n",
        "# 3. Select the feature with the highest information gain\n",
        "# 4. Split the dataset based on the selected feature\n",
        "# 5. Recursively build the decision tree\n",
        "# 6. stopping criteria"
      ],
      "metadata": {
        "id": "CPQYItIJSPyR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from math import log2\n",
        "\n",
        "def entropy(y):\n",
        "    \"\"\"Calculate the entropy of a dataset.\"\"\"\n",
        "    class_counts = Counter(y)\n",
        "    total_instances = len(y)\n",
        "    entropy = 0\n",
        "    for count in class_counts.values():\n",
        "        prob = count / total_instances\n",
        "        entropy -= prob * log2(prob)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "# apply the entropy function on an dummy data\n",
        "\n",
        "X = { # use tuple instead of list for the set\n",
        "    ('sunny', 'hot', 'high', 'weak'),\n",
        "    ('sunny', 'hot', 'high', 'strong'),\n",
        "    ('overcast', 'hot', 'high', 'weak'),\n",
        "    ('rain', 'mild', 'high', 'weak'),\n",
        "    ('rain', 'cool', 'normal', 'weak')\n",
        "}\n",
        "\n",
        "y = ( # use tuple instead of list\n",
        "     'no', 'no', 'yes', 'yes', 'yes'\n",
        ")\n",
        "\n",
        "\n",
        "entropy(y)\n",
        "\n",
        "# 0.97 is entropy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acuUUKistUq6",
        "outputId": "e6d0e248-3b95-431c-e3b1-d2ee8394a7ca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9709505944546686"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data = {\n",
        "    'X' : ['A' , 'B' , 'C' , 'D' , 'E', 'F' , 'G' , 'H' , 'I ', 'J'],\n",
        "    # only 2 no all yes\n",
        "    'y' : ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes','Yes','No','Yes','No']\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(Data)\n",
        "df\n",
        "\n",
        "X = df['X']\n",
        "y = df['y']\n",
        "\n",
        "prob_yes = len([i for i in y if i == 'Yes']) / len(y)\n",
        "prob_no = len([i for i in y if i == 'No']) / len(y)\n",
        "print(prob_yes)\n",
        "print(prob_no)\n",
        "\n",
        "entropy(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85jgc3WrtgBs",
        "outputId": "80c26dc5-1fce-41f1-be4d-724e9daa0248"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6\n",
            "0.4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9709505944546686"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate infromation gain\n",
        "\n",
        "\n",
        "# suppose atrribute A has two attribute A1 and A2\n",
        "\n",
        "\n",
        "# A1 has 4 instances(3 \"yes\" , 1 \"no\") and A2 has 4 instances(3 \"yes\" , 3 \"no\")\n",
        "\n",
        "\n",
        "# calculate A1 and A2 entropy\n",
        "# lets create A1 and A2 data\n",
        "\n",
        "A1 = {\n",
        "    'X' : ['A' , 'B' , 'C' , 'D'],\n",
        "    'y' : ['Yes', 'No', 'Yes', 'Yes']\n",
        "}\n",
        "\n",
        "A2 = {\n",
        "    'X' : ['E' , 'F' , 'G' , 'H','I','J'],\n",
        "    'y' : ['Yes', 'No', 'Yes', 'No','Yes','No']\n",
        "}\n",
        "\n",
        "df1 = pd.DataFrame(A1)\n",
        "df2 = pd.DataFrame(A2)\n",
        "y1 = df1['y']\n",
        "y2 = df2['y']\n",
        "print(entropy(y1))\n",
        "print(entropy(y2))\n",
        "# calculate  Weighted average entropy after split\n",
        "Hsplit = prob_no * entropy(y1) + prob_yes * entropy(y2)\n",
        "print(Hsplit)\n",
        "# Information gain\n",
        "IG = entropy(y) - Hsplit\n",
        "print(IG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-P6c1GHw6_A",
        "outputId": "814ce0bc-4648-4ea4-f38f-41c34a40d341"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8112781244591328\n",
            "1.0\n",
            "0.9245112497836532\n",
            "0.0464393446710154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DT:\n",
        "  def __init__(self):\n",
        "    self.tree = None\n",
        "\n",
        "  def entropy(y):\n",
        "    \"\"\"Calculate the entropy of a dataset.\"\"\"\n",
        "    class_counts = Counter(y)\n",
        "    total_instances = len(y)\n",
        "    entropy = 0\n",
        "    for count in class_counts.values():\n",
        "        prob = count / total_instances\n",
        "        entropy -= prob * log2(prob)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "  def split_data(X, y, feature, value):\n",
        "    \"\"\"Split the dataset based on the selected feature and value.\"\"\"\n",
        "    X_subset = []\n",
        "    y_subset = []\n",
        "    for i in range(len(X)):\n",
        "        if X[i][feature] == value:\n",
        "            X_subset.append(X[i])\n",
        "            y_subset.append(y[i])\n",
        "    return X_subset, y_subset\n",
        "\n",
        "  # calculate entropy after split for each subset\n",
        "  def entropy_after_split(X_subset, y_subset):\n",
        "    # calculate for both\n",
        "    for subset in X_subset:\n",
        "        Entropy_X_sub = (entropy(subset))\n",
        "    return Entropy_X_sub\n",
        "\n",
        "    for subset in y_subset:\n",
        "        Entropy_y_sub = (entropy(subset))\n",
        "    return Entropy_y_sub\n",
        "\n",
        "\n",
        "    # calculate weighted average\n",
        "  def weighted_average(prob_yes , prob_no , y):\n",
        "    Hsplit = prob_no * entropy(y1) + prob_yes * entropy(y2)\n",
        "    return Hsplit\n",
        "\n",
        "\n",
        "  def information_gain(y , Hsplit):\n",
        "    IG = entropy(y) - Hsplit\n",
        "    return IG\n",
        "\n",
        "\n",
        "Data = {\n",
        "    'X' : ['A' , 'B' , 'C' , 'D' , 'E', 'F' , 'G' , 'H' , 'I ', 'J'],\n",
        "    # only 2 no all yes\n",
        "    'y' : ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes','Yes','No','Yes','No']\n",
        "}\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(Data)\n",
        "X = df['X']\n",
        "y = df['y']\n",
        "prob_yes = len([i for i in y if i == 'Yes']) / len(y)\n",
        "prob_no = len([i for i in y if i == 'No']) / len(y)\n",
        "\n",
        "\n",
        "\n",
        "Des = DT()\n",
        "Des.entropy(y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "fCLzZ19Eypbi",
        "outputId": "83d2a426-595d-4301-845f-7b7e395e4b8a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "DT.entropy() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-a4ec6b01fb0d>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mDes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mDes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DT.entropy() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from math import log2\n",
        "import graphviz\n",
        "\n",
        "class DT:\n",
        "    def __init__(self):\n",
        "        self.tree = None\n",
        "\n",
        "    def entropy(self, y):\n",
        "        \"\"\"Calculate the entropy of a dataset.\"\"\"\n",
        "        class_counts = Counter(y)\n",
        "        total_instances = len(y)\n",
        "        entropy = 0\n",
        "        for count in class_counts.values():\n",
        "            prob = count / total_instances\n",
        "            entropy -= prob * log2(prob)\n",
        "        return entropy\n",
        "\n",
        "    def split_data(self, X, y, feature):\n",
        "        \"\"\"Split the dataset based on the unique values of the selected feature.\"\"\"\n",
        "        unique_values = set(X[feature])\n",
        "        subsets = {}\n",
        "        for value in unique_values:\n",
        "            X_subset = []\n",
        "            y_subset = []\n",
        "            for i in range(len(X)):\n",
        "                if X[feature].iloc[i] == value:\n",
        "                    X_subset.append(X.iloc[i])\n",
        "                    y_subset.append(y.iloc[i])\n",
        "            subsets[value] = (X_subset, y_subset)\n",
        "        return subsets\n",
        "\n",
        "    def entropy_after_split(self, y_subsets):\n",
        "        \"\"\"Calculate the entropy after the split for each subset.\"\"\"\n",
        "        total_instances = sum([len(y_subset) for y_subset in y_subsets.values()])\n",
        "        weighted_entropy = 0\n",
        "        for y_subset in y_subsets.values():\n",
        "            prob = len(y_subset) / total_instances\n",
        "            weighted_entropy += prob * self.entropy(y_subset)\n",
        "        return weighted_entropy\n",
        "\n",
        "    def information_gain(self, y, y_subsets):\n",
        "        \"\"\"Calculate the information gain.\"\"\"\n",
        "        Hsplit = self.entropy_after_split(y_subsets)\n",
        "        IG = self.entropy(y) - Hsplit\n",
        "        return IG\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        \"\"\"Find the best feature to split on.\"\"\"\n",
        "        best_ig = -1\n",
        "        best_feature = None\n",
        "        for feature in X.columns:\n",
        "            subsets = self.split_data(X, y, feature)\n",
        "            y_subsets = {k: v[1] for k, v in subsets.items()}\n",
        "            ig = self.information_gain(y, y_subsets)\n",
        "            if ig > best_ig:\n",
        "                best_ig = ig\n",
        "                best_feature = feature\n",
        "        return best_feature, best_ig\n",
        "\n",
        "    def visualize_tree(self):\n",
        "        def add_nodes_edges(tree, graph, parent=None, label=None):\n",
        "            if isinstance(tree, dict):\n",
        "                for key, value in tree.items():\n",
        "                    if parent is not None:\n",
        "                        graph.node(key)\n",
        "                        graph.edge(parent, key, label=label)\n",
        "                    add_nodes_edges(value, graph, parent=key, label=str(key))\n",
        "            else:\n",
        "                graph.node(str(tree), shape='ellipse')\n",
        "                if parent is not None:\n",
        "                    graph.edge(parent, str(tree), label=label)\n",
        "\n",
        "        dot = graphviz.Digraph()\n",
        "        add_nodes_edges(self.tree, dot)\n",
        "        return dot\n",
        "\n",
        "# Example usage with your dataset\n",
        "Data = {\n",
        "    'X': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
        "    'y': ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No']\n",
        "}\n",
        "df = pd.DataFrame(Data)\n",
        "\n",
        "X = df[['X']]  # Note that X needs to be a DataFrame, not a Series\n",
        "y = df['y']\n",
        "\n",
        "dt = DT()\n",
        "\n",
        "# Calculate the entropy of the dataset\n",
        "print(\"Entropy of dataset:\", dt.entropy(y))\n",
        "\n",
        "# Find the best feature to split on\n",
        "best_feature, best_ig = dt.best_split(X, y)\n",
        "print(\"Best feature to split on:\", best_feature)\n",
        "print(\"Information Gain:\", best_ig)\n",
        "\n",
        "# Split the data based on the best feature\n",
        "subsets = dt.split_data(X, y, best_feature)\n",
        "for value, (X_subset, y_subset) in subsets.items():\n",
        "    print(f\"Value: {value}\")\n",
        "    print(f\"X subset: {X_subset}\")\n",
        "    print(f\"y subset: {y_subset}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYRYwzIU2AEY",
        "outputId": "c5ffcfcc-8b7c-440c-bdda-d40419707495"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of dataset: 0.9709505944546686\n",
            "Best feature to split on: X\n",
            "Information Gain: 0.9709505944546686\n",
            "Value: B\n",
            "X subset: [X    B\n",
            "Name: 1, dtype: object]\n",
            "y subset: ['No']\n",
            "Value: H\n",
            "X subset: [X    H\n",
            "Name: 7, dtype: object]\n",
            "y subset: ['No']\n",
            "Value: C\n",
            "X subset: [X    C\n",
            "Name: 2, dtype: object]\n",
            "y subset: ['Yes']\n",
            "Value: F\n",
            "X subset: [X    F\n",
            "Name: 5, dtype: object]\n",
            "y subset: ['Yes']\n",
            "Value: D\n",
            "X subset: [X    D\n",
            "Name: 3, dtype: object]\n",
            "y subset: ['Yes']\n",
            "Value: A\n",
            "X subset: [X    A\n",
            "Name: 0, dtype: object]\n",
            "y subset: ['Yes']\n",
            "Value: E\n",
            "X subset: [X    E\n",
            "Name: 4, dtype: object]\n",
            "y subset: ['No']\n",
            "Value: I\n",
            "X subset: [X    I\n",
            "Name: 8, dtype: object]\n",
            "y subset: ['Yes']\n",
            "Value: J\n",
            "X subset: [X    J\n",
            "Name: 9, dtype: object]\n",
            "y subset: ['No']\n",
            "Value: G\n",
            "X subset: [X    G\n",
            "Name: 6, dtype: object]\n",
            "y subset: ['Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn implementation\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "Data = {\n",
        "    'X': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
        "    'y': ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No']\n",
        "}\n",
        "df = pd.DataFrame(Data)\n",
        "\n",
        "\n",
        "# encode the data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df = df.apply(le.fit_transform)\n",
        "\n",
        "X = df[['X']]\n",
        "y = df['y']\n",
        "\n",
        "\n",
        "DecT = DecisionTreeClassifier()\n",
        "DecT.fit(X, y)\n",
        "predictions = DecT.predict(X)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivt4bdQs37VX",
        "outputId": "ac3fa639-95e2-47c4-a1e1-0173c1bbd46c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 1 0 1 1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovUkxC-I5Koi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}